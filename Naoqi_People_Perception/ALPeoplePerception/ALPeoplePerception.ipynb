{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALPeoplePerception\n",
    "___\n",
    "## What it does\n",
    "___\n",
    "\n",
    "=> ALPeoplePerception 은 로봇 주위의 사람들을 지속적으로 따라다니며, 그들에 대한 기본적인 정보를 제공하는 extracter이다. 이것은 RGB Camera와 가능하다면 3D sensor를 통해서 시각적 정보를 모은다. \n",
    "\n",
    "=> 일단 사람들이 detected 되면, ALPeoplePerception에 의해 그들의 속성들이 지속적으로 업데이트 됩니다. 수집된 모든 정보들은 ALMemory key와 발생된 이벤트들을 통해 사용할 수 있게 된다. \n",
    "\n",
    "#### ALMemory keys examples:\n",
    "1. FRAME_TORSO frame 과 관련이 있는 공간의 위치\n",
    "2. 얼마나 detected되어졌는지\n",
    "\n",
    "#### Raised events examples:\n",
    "1. 새로운 사람이 detected 된 경우\n",
    "2. 사람이 사라졌다고 사료된 경우\n",
    "\n",
    "=> 이에 해당되는 모든 memory key들과 event들은 (http://doc.aldebaran.com/2-4/naoqi/peopleperception/alpeopleperception-api.html#alpeopleperception-api) 에 문서화 되어있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works\n",
    "___\n",
    ": ALPeoplePerception 은 시각정 단서들을 사용하여, 로봇 주변의 잠재적인 인산들을 찾아내려 시도한다.\n",
    "\n",
    "### People list\n",
    ": 현재 video frame 에서 detected 된 모든 새로운 사람들은 이전에 알게된 사람들과 (가능한 경우)연관됩니다. 이것은 누군가를 추적하고 그의 속성들을 업데이트하는 것에 도움이 된다. 만약 과거에 알게된 사람이 새로운 사람과 연관될 수 없는 경우, 그 사람은 모집단(population)에 추가된다. \n",
    "\n",
    "### Visible / Non visible people lists\n",
    ": 어떤 사람이 시야에서 벗어나게 된 경우, 사라짐이 일시적이거나 로봇의 움직임들의 결과일 수 있으므로, 그/그녀는 people list에서 즉각적으로 제거되지는 않는다.\n",
    "\n",
    "그러므로, People list는 두가지 하위 목록으로 나뉜다.\n",
    "\n",
    "1. 보이는 사람들의 list\n",
    "2. 보이지 않는 사람들의 list\n",
    "\n",
    "누군가 잠시동안 보이지 않을때, 그 혹은 그녀는 현재의 모집단에서 제거될 것이다. \n",
    "\n",
    "두가지 방법을 사용하면 한사람이 모집단으로부터 제거되어지기 까지의 시간을 set 할 수 있다.\n",
    "1. ALPeoplePerception::setTimeBeforePersonDisappears\n",
    "2. ALPeoplePerception::setTimeBeforeVisiblePersonDisappears\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performances and limitations\n",
    "___\n",
    ": detection절차는 detection을 향상시키는 추가적인 기술들로부터 이점을 얻을 수 있지만, 그들은 일반적으로 더 많은 CPU 자원들을 필요로한다. ALPeoplePerception::setFastModeEnabled을 calling함으로써, 이것을 enable or disabled 할 수 있다. 현재는 movement detection만 fast mode의 영향을 받는다..\n",
    "\n",
    ": processing resource들을 저장하는 또다른 방법으로는 detection과 tracking 범위를 제한 하는 것이 있다. 로봇이 근거리 상호작용이 있을때 특히 흥미롭다. 그 이유는 항상 동시에 많은 사람들을 볼 필요가 없고, 예를 들면 TTS 혹은 dialog tasks를 위해 전력을 절약해야하기 때문이다.\n",
    "\n",
    "see ALPeoplePerception::setMaximumDetectionRange (http://doc.aldebaran.com/2-4/naoqi/peopleperception/alpeopleperception-api.html#ALPeoplePerception::setMaximumDetectionRange__floatCR)\n",
    "\n",
    ">Warning : 로봇의 머리 위치와 관련이 있는 위치가 포함되어 있으므로 로봇이 움직일 때 그 값이 잘못될 수 있다.\n",
    "\n",
    "### Known Limitations\n",
    ": 다음 조건들은 올바른 detection을 방해한다.\n",
    "\n",
    "1. 매우 강한 backlight. 특히 카메라가 햇빛을 직접 받는 경우.\n",
    "2. 사람이 벽, 사람 사이즈의 물체 혹은 다른 사람과 같은 것이 너무 가까이 서 있는 경우.\n",
    "\n",
    "=> 보다 나은 detection을 위해서는 이러한 조건을 피하는 것이 좋다.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
